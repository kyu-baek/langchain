ì•ˆë…•í•˜ì„¸ìš”. 
ë­ì²´ì¸ì— ê´€ì‹¬íˆ ë§ì€ ë°±ì—”ë“œ ê°œë°œì ì…ë‹ˆë‹¤.
ì˜¤ëŠ˜ ì œê°€ ìµœì¢…ì ìœ¼ë¡œ ì†Œê°œí•´ë“œë¦´ ì„œë¹„ìŠ¤ëŠ” ë­ì²´ì¸ì˜ í™œìš©í•œ mbti ì¸¡ì •ê¸° ì…ë‹ˆë‹¤.

í•´ë‹¹ ì†ŒìŠ¤ ì½”ë“œëŠ” Streamlit.py ì…ë‹ˆë‹¤.

ë°°í¬ëœ Streamlit ì‚¬ì´íŠ¸ ë„ë©”ì¸ì…ë‹ˆë‹¤.

https://langchain-romina.streamlit.app/


ì²˜ìŒ ë­ì²´ì¸ì„ ì ‘í•˜ë©´ì„œ ìƒˆë¡œìš´ ê°œë…ì— ì ì‘í•˜ëŠë¼ ê³ êµ°ë¶„íˆ¬í–ˆìŠµë‹ˆë‹¤. 

ê°„ë‹¨í•˜ê²Œ openAi ì— ì§ˆë¬¸ì„ ë³´ë‚´ëŠ” ê²ƒë¶€í„° ì‹œì‘í•´, ì—¬ëŸ¬ê°€ì§€ ë­ì²´ì¸ ê¸°ëŠ¥ì„ í™œìš©í•´ë³´ì•˜ìŠµë‹ˆë‹¤.
ì œê°€ ë­ì²´ì¸ì„ ê³µë¶€í•œ ì—¬ì •ì„ ì •ë¦¬í•´ì„œ ê³µìœ í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.
ì½”ë“œëŠ” íŒŒì´ì¬ìœ¼ë¡œ ì§°ì§€ë§Œ, ì €ëŠ” íŒŒì´ì¬ì„ ê·¸ë ‡ê²Œ ì˜ ë‹¤ë£¨ì§€ëŠ” ëª»í•©ë‹ˆë‹¤. 
ê·¸ë˜ì„œ ì½”ë“œê°€ ì¡°ì•…í•œ ì ì€ ì–‘í•´ë¶€íƒë“œë¦½ë‹ˆë‹¤.


ë‹¨ê³„ë³„ë¡œ 

OpenAi ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

ChatOpenAi ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

openAi ì‚¬ìš© ìš”ê¸ˆ ì¡°íšŒ

ì—°ê²°ëœ ì§ˆë¬¸ì„ ê¸°ì–µí•˜ëŠ” ëŒ€í™” ì²´ì¸ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

Summary ì²´ì¸ì„ ì‚¬ìš©í•´ ê¸€ì„ ìš”ì•½í•˜ëŠ” ì˜ˆì‹œ

output parser ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

ë©€í‹° ì²´ì¸ì„ ì‚¬ìš©í•´ mbti ì¸¡ì •ê¸° ì„œë¹„ìŠ¤ ë¡œì§ êµ¬ì„± ì˜ˆì‹œ



OpenAi ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

from langchain.llms import OpenAI

llm = OpenAI(temperature=0.9)
text = "what would be a good company name for a company that makes colorful socks?"
print(llm(text))


ChatOpenAi ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(temperature=0.8)

# ì—¬ëŸ¬ê°œë¡œ ì œë„ˆë ˆì´íŒ… ê°€ëŠ¥
more_messages = [
[
    SystemMessage(content="You are a helpful assistant that translate English to Korean"),
    HumanMessage(content="Translate this sentence from English to Korean. I love programming.")
],
[
    SystemMessage(content="You are a helpful assistant that translate English to Korean"),
    HumanMessage(content="Translate this sentence from English to Korean. I hate programming.")
],
[
    SystemMessage(content="You are a helpful assistant that translate English to Korean"),
    HumanMessage(content="Translate this sentence from English to Korean. I do programming.")
]
]

result = chat.generate(more_messages)


**output**

generations=[[ChatGeneration(text='ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì‚¬ë‘í•©ë‹ˆë‹¤.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì‚¬ë‘í•©ë‹ˆë‹¤.', additional_kwargs={}, example=False))], [ChatGeneration(text='ë‚˜ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì‹«ì–´í•´ìš”.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='ë‚˜ëŠ” í”„ë¡œê·¸ë˜ë°ì„ ì‹«ì–´í•´ìš”.', additional_kwargs={}, example=False))], [ChatGeneration(text='ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ í•©ë‹ˆë‹¤.', generation_info={'finish_reason': 'stop'}, message=AIMessage(content='ì €ëŠ” í”„ë¡œê·¸ë˜ë°ì„ í•©ë‹ˆë‹¤.', additional_kwargs={}, example=False))]] llm_output={'token_usage': {'prompt_tokens': 99, 'completion_tokens': 40, 'total_tokens': 139}, 'model_name': 'gpt-3.5-turbo'} run=[RunInfo(run_id=UUID('4d03f852-e940-4c22-935a-ed58ff8fa956')), RunInfo(run_id=UUID('e3703ca4-92df-4da6-90fa-dd835f8e8c5e')), RunInfo(run_id=UUID('51d9e894-44a2-4704-b67a-d38b953efab4'))]
print(result)



3. openAi ì‚¬ìš© ìš”ê¸ˆ ì¡°íšŒ


from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback

chat = OpenAI(model_name='gpt-3.5-turbo')


# chat("1980ë…„ëŒ€ ë©”íƒˆ ìŒì•… 5ê³¡ ì¶”ì²œí•´ì¤˜.")
with get_openai_callback() as cb:

  result = chat("1980ë…„ëŒ€ ë©”íƒˆ ìŒì•… 5ê³¡ ì¶”ì²œí•´ì¤˜.")

  print(f"Total Tokens: {cb.total_tokens}")
  print(f"Prompt Tokens: {cb.prompt_tokens}")
  print(f"Completion Tokens: {cb.completion_tokens}")
  print(f"Total Cost (USD): ${cb.total_cost}")
  print(cb)

result



ì—°ê²°ëœ ì§ˆë¬¸ì„ ê¸°ì–µí•˜ëŠ” ëŒ€í™” ì²´ì¸ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

from langchain import ConversationChain
from langchain.llms import OpenAI
from langchain.memory import ConversationBufferMemory

llm = OpenAI(temperature=0)

conversation = ConversationChain(
    llm=llm, verbose=True, memory=ConversationBufferMemory()
)

conversation.predict(input="ì¸ê³µì§€ëŠ¥ì—ì„œ Transformer ê°€ ë­ì•¼?")
print(conversation.memory)
print("\n")

conversation.predict(input="RNNê³¼ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì¤˜")
print(conversation.memory)

Summary ì²´ì¸ì„ ì‚¬ìš©í•´ ê¸€ì„ ìš”ì•½í•˜ëŠ” ì˜ˆì‹œ

from langchain import OpenAI
from langchain.chains.summarize import load_summarize_chain

llm = OpenAI(model_name="gpt-3.5-turbo", temperature=0)

# stuff: LLM í•œë²ˆì— ë‹¤ë³´ëƒ„. ê¸¸ë©´ ì˜¤ë¥˜
# map_reduce : ë‚˜ëˆ ì„œ ìš”ì•½ í›„ í•©ì³ì„œ ë‹¤ì‹œ ìš”ì•½
# refine :(ìš”ì•½ + ë‹¤ìŒë¬¸ì„œ) =>ìš”ì•½
# map_rerank: ì ìˆ˜ë¥¼ ë§¤ê²¨ì„œ ì¤‘ìš”í•œê±°ë¡œ ìš”ì•½
summary_chain = load_summarize_chain(llm, chain_type="map_reduce")

from langchain.chains import AnalyzeDocumentChain

summary_doc_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)

with open('chat.txt') as f:
    chat = f.read()

print(summary_doc_chain.run(chat))

output parser ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import ChatPromptTemplate
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str):
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()

# ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ë°ì´í„° ë³´í˜¸ë¥¼ ìœ„í•´ Invoke í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.
chain.invoke({"text": "colors"})

**output**

['red', 'blue', 'green', 'yellow', 'orange']



7. ë©€í‹° ì²´ì¸ì„ ì‚¬ìš©í•´ mbti ì¸¡ì •ê¸° ì„œë¹„ìŠ¤ ë¡œì§ êµ¬ì„± ì˜ˆì‹œ with Streamlit

from langchain.chains import SequentialChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import ChatPromptTemplate
from langchain.chains import LLMChain
import streamlit as st
import os



api_key = st.sidebar.text_input('OpenAI API Key', type='password')
os.environ['OPENAI_API_KEY'] = api_key
if api_key:

    llm = ChatOpenAI(temperature=0.8, model="gpt-3.5-turbo-0613")


    # 1. input ê°’ ìš”ì•½í•˜ê¸°
    first_prompt = ChatPromptTemplate.from_template(
        "Summarize the content:"
        "\n\n{Content}"
    )
    chain_one = LLMChain(llm=llm, prompt=first_prompt, 
                        output_key="summary"
                        )


    # 2. ê°ì •ìƒíƒœ ì´ëª¨ì§€ë¡œ í‘œí˜„í•˜ê¸°
    second_prompt = ChatPromptTemplate.from_template(
        "Chose one of the emogis in the Emogi that repersent writer's sentiment"
        "\n\n{summary}"
    )
    chain_two = LLMChain(llm=llm, prompt=second_prompt, 
                        output_key="sentiment"
                        )

    # 3. MBTI ì¶”ì¸¡í•˜ê¸°
    third_prompt = ChatPromptTemplate.from_template(
        "Chose one of the Overall MBTI types that repersent writer"
        "\n\n{summary}"
    )
    chain_three = LLMChain(llm=llm, prompt=third_prompt, 
                        output_key="mbti"
                        )



    # 1,2,3ë²ˆ ì²´ì¸ì„ ë¬¶ìŒ
    overall_chain = SequentialChain(
        chains=[chain_one, chain_two, chain_three],
        input_variables=["Content"],
        output_variables=["summary", "sentiment", "mbti"],
    )



from langchain.callbacks import get_openai_callback
import re

st.title("MBTI ì¸¡ì •ê¸°")
Content = st.text_area("ì•„ì§ ì¹œí•˜ì§„ ì•Šì§€ë§Œ ë‹¹ì‹ ì´ ê¶ê¸ˆí•´í•˜ëŠ” ì‚¬ëŒì— ëŒ€í•´ì„œ ì•Œë ¤ë“œë¦´ê²Œìš”. ê·¸ ì‚¬ëŒì´ ì‘ì„±í•œ sns ê¸€ì„ ì˜¬ë ¤ì£¼ì„¸ìš”", height=200, max_chars=700)


if st.button('ì•Œì•„ê°€ê¸°'):
    with get_openai_callback() as cb:
        result = overall_chain(Content, return_only_outputs=True)

    summary = result['summary']
    sentiment = result['sentiment']
    mbti = result['mbti']

    # ì •ê·œ í‘œí˜„ì‹ì„ ì´ìš©í•˜ì—¬ ì²˜ìŒìœ¼ë¡œ ë‚˜ì˜¤ëŠ” MBTI ìœ í˜•ì„ ì°¾ìŒ
    mbti_types = ["ISTJ", "ISFJ", "INFJ", "INTJ", "ISTP", "ISFP", "INFP", "INTP", 
                "ESTP", "ESFP", "ENFP", "ENTP", "ESTJ", "ESFJ", "ENFJ", "ENTJ"]

    pattern = r"\b(" + "|".join(mbti_types) + r")\b"
    mbti_result = re.search(pattern, mbti, re.IGNORECASE)

    st.subheader('summary')
    st.write(summary)
    st.divider()
    st.subheader('Predict :blue[MBTI]:')

    # ì°¾ì€ MBTI ê²°ê³¼ë¥¼ ì €ì¥
    if mbti_result:
        extracted_mbti = mbti_result.group(1).upper()  # ëŒ€ë¬¸ìë¡œ ë³€í™˜
        st.header(extracted_mbti)
        st.write(mbti)
    else:
        st.write(mbti)
    st.divider()
    st.subheader('Predict :blue[Feeling now]:')
    st.title(sentiment)
    st.divider()
    st.info(cb)




MBTI ì¸¡ì •ê¸°ë¥¼ ë§Œë“¤ì–´ ë³´ì•˜ìœ¼ë‹ˆ, ì–¼ë§ˆë‚˜ ì˜ ì¸¡ì •í•´ì£¼ëŠ”ì§€ í…ŒìŠ¤íŠ¸ë¥¼ í•´ì•¼ê² ì£ ?
ì•„ë˜ëŠ” ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ ì¸í„°ë·° ê¸€ì…ë‹ˆë‹¤. 

https://www.ft.com/content/697d8d32-6ef9-4b4c-835a-3e9dcbdb431a


ì¼ì • ë¶€ë¶„ ë°œì·Œí•´ì„œ MBTI í…ŒìŠ¤íŠ¸ì— ë„£ì–´ë³´ê² ìŠµë‹ˆë‹¤.



ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤!!


ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ ì¸í„°ë·°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸¡ì •í•œ ê²°ê³¼ ê·¸ì˜ MBTI ëŠ” INTJ ì…ë‹ˆë‹¤.



ë‹¤í–‰ì´ ì‹¤ì œ MBTI ì™€ ì¼ì¹˜í•˜ë„¤ìš” ğŸ˜„


